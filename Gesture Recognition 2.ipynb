{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5f8462",
   "metadata": {},
   "source": [
    "# Task 2: Understand body language by gesture recognition with convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fb5729",
   "metadata": {},
   "source": [
    "## 1. Do literature search on Convolution Neural Network. Learn how to build a convolutional layer in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd73ca5b",
   "metadata": {},
   "source": [
    "## 2. Referring to the guide in Task 1, build your own network for gesture classification using convolutional layers. Please see the references 4 in the manual to learn how to build convolutional layers in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15568252",
   "metadata": {},
   "source": [
    "## 3. Analyse and comment on the performance of the model. Make a comparison between the fully connected based and convolutional based models and comment on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa70e87",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd740f",
   "metadata": {},
   "source": [
    "##  1.Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c7e6b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# for creating train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as utils_data\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f80ab53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reading and displaying images\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a91750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import itertools\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f679c4a",
   "metadata": {},
   "source": [
    "## 2.Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa08f277",
   "metadata": {},
   "source": [
    "Define a function to preprocess the images including resizing and binaryzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a6ea851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSkinImage(filePath, resize_HW=48):\n",
    "    # step 1\n",
    "    # read the image\n",
    "    original = cv2.imread(filename=filePath)\n",
    "\n",
    "    # step 2\n",
    "    # resize the image to\n",
    "    image_resized = cv2.resize(original, (resize_HW, resize_HW))\n",
    "\n",
    "    # step 3\n",
    "    # convert the image from rgb to YCbCr\n",
    "    image_ycbcr = cv2.cvtColor(image_resized, cv2.COLOR_BGR2YCR_CB)\n",
    "\n",
    "    # step 4\n",
    "    # get the central color of the image\n",
    "    # expected the hand to be in the central of the image\n",
    "    Cb_center_color = image_ycbcr[int(resize_HW/2), int(resize_HW/2), 1]\n",
    "    Cr_center_color = image_ycbcr[int(resize_HW/2), int(resize_HW/2), 2]\n",
    "    # set the range\n",
    "    Cb_Difference = 15\n",
    "    Cr_Difference = 10\n",
    "\n",
    "    # step 5\n",
    "    # detect skin pixels\n",
    "    Cb = image_ycbcr[:, :, 1]\n",
    "    Cr = image_ycbcr[:, :, 2]\n",
    "    index = np.where((Cb >= Cb_center_color-Cb_Difference) & (Cb <= Cb_center_color+Cb_Difference)\n",
    "                     & (Cr >= Cr_center_color-Cr_Difference) & (Cr <= Cr_center_color+Cr_Difference))\n",
    "\n",
    "    # Mark detected pixels and output\n",
    "    image_output = np.zeros((resize_HW, resize_HW))\n",
    "    image_output[index] = 255\n",
    "\n",
    "    # show image\n",
    "    # cv2.imshow(\"\", image_output)\n",
    "    # cv2.waitKey(0)\n",
    "    return image_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af35bb1",
   "metadata": {},
   "source": [
    "Deal with all the images using the function defined above. The processed data is stored in a new folder 'dataset_processed'.\n",
    "\n",
    "Generate labels for each class. (class 0, 1, ..., num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f179545",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './dataset/images'\n",
    "path_processed = './dataset_processed/images'\n",
    "\n",
    "# -------------------images processing--------------\n",
    "for mainDir, subDir, fileList in os.walk(path):\n",
    "    for file in fileList:\n",
    "        currentPath = os.path.join(mainDir, file)\n",
    "        processedImage = processSkinImage(currentPath)\n",
    "\n",
    "        new_mainDir = path_processed + mainDir.split(path)[-1]\n",
    "        if not os.path.exists(new_mainDir):\n",
    "            os.makedirs(new_mainDir)\n",
    "        cv2.imwrite(os.path.join(new_mainDir, file), processedImage)\n",
    "\n",
    "# -----------------label generation----------------\n",
    "label_path = './dataset_processed/labels'\n",
    "if not os.path.exists(label_path):\n",
    "    os.makedirs(label_path)\n",
    "\n",
    "files = os.listdir(path)\n",
    "for i, file in enumerate(files):\n",
    "    subclass_label_path = os.path.join(label_path, file+'.txt')\n",
    "    with open(subclass_label_path, 'w') as f:\n",
    "        f.write('#label\\n')\n",
    "    for _ in range(len(os.listdir(os.path.join(path_processed, file)))):\n",
    "        with open(subclass_label_path, 'a') as f:\n",
    "            f.write('{:d}\\n'.format(i))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d27ea",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e159eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image = [] #x\n",
    "path_images = './dataset_processed/images'\n",
    "for mainDir, subDir, fileList in os.walk(path_images):\n",
    "    for file in fileList:\n",
    "        currentPath = os.path.join(mainDir, file)\n",
    "        Image.append(cv2.imread(currentPath)[:, :, 0])\n",
    "Image = np.array(Image)\n",
    "dataset_size, H, W = Image.shape\n",
    "# for FCNN model, the image need to be stretched into one dimension: (b, h, w)->(b, h*w)\n",
    "Image = Image.reshape(dataset_size, 1, H, W)\n",
    "\n",
    "\n",
    "Label = []  #y\n",
    "path_labels = './dataset_processed/labels'\n",
    "for file in os.listdir(path_labels):\n",
    "    Label.append(np.loadtxt(os.path.join(path_labels, file)))\n",
    "Label = np.array(list(itertools.chain.from_iterable(Label)))\n",
    "num_classes = int(np.max(Label))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1012632a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images loaded:  78\n",
      "Labels loaded:  78\n"
     ]
    }
   ],
   "source": [
    "print(\"Images loaded: \", len(Image))\n",
    "print(\"Labels loaded: \", len(Label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b610c",
   "metadata": {},
   "source": [
    "##  3. Create train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5a62a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_classes = 4\n",
    "batch_size = 25\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "050b4f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ready!\n"
     ]
    }
   ],
   "source": [
    "dataset = utils_data.TensorDataset(torch.Tensor(Image), torch.LongTensor(Label))\n",
    "split_ratio = 0.8\n",
    "train_size = int(split_ratio * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "train_set, test_set = utils_data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = utils_data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = utils_data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n",
    "print('Data is ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de107f49",
   "metadata": {},
   "source": [
    "##  4. Implementing CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af9e6014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5, stride=1, padding=0) #1,4\n",
    "        self.relu1 = nn.ReLU()        \n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        # Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=5, stride=1, padding=0) #4,8\n",
    "        self.relu2 = nn.ReLU()      \n",
    "        # Max pool 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        # Fully connected 1 (readout)\n",
    "        self.fc1 = nn.Linear(648, 25) #8,9,9\n",
    "        self.fc2 = nn.Linear(25, 4) #8,9,9\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolution 1\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "        # Max pool 1\n",
    "        out = self.maxpool1(out)\n",
    "        # Convolution 2 \n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "        # Max pool 2 \n",
    "        out = self.maxpool2(out)     \n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "        x= self.fc2(out)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf41921",
   "metadata": {},
   "source": [
    "## 5. Loss and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7629ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0982bb7",
   "metadata": {},
   "source": [
    "## 6. Train and Test the data using CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eeaec5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0\ttrain loss=65.900239\ttrain accuracy=0.387\ttest accuracy=0.250\n",
      "epoch=1\ttrain loss=2.269722\ttrain accuracy=0.484\ttest accuracy=0.688\n",
      "epoch=2\ttrain loss=0.612733\ttrain accuracy=0.726\ttest accuracy=0.938\n",
      "epoch=3\ttrain loss=0.169063\ttrain accuracy=0.968\ttest accuracy=0.938\n",
      "epoch=4\ttrain loss=0.083873\ttrain accuracy=0.984\ttest accuracy=1.000\n",
      "epoch=5\ttrain loss=0.022070\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=6\ttrain loss=0.095359\ttrain accuracy=0.984\ttest accuracy=0.938\n",
      "epoch=7\ttrain loss=0.009064\ttrain accuracy=1.000\ttest accuracy=1.000\n",
      "epoch=8\ttrain loss=0.018255\ttrain accuracy=0.984\ttest accuracy=0.938\n",
      "epoch=9\ttrain loss=0.004835\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=10\ttrain loss=0.002859\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=11\ttrain loss=0.002915\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=12\ttrain loss=0.001990\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=13\ttrain loss=0.001572\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=14\ttrain loss=0.000933\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=15\ttrain loss=0.000672\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=16\ttrain loss=0.000375\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=17\ttrain loss=0.000382\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=18\ttrain loss=0.000276\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=19\ttrain loss=0.000214\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=20\ttrain loss=0.000213\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=21\ttrain loss=0.000196\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=22\ttrain loss=0.000149\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=23\ttrain loss=0.000151\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=24\ttrain loss=0.000151\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=25\ttrain loss=0.000165\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=26\ttrain loss=0.000133\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=27\ttrain loss=0.000118\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=28\ttrain loss=0.000127\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=29\ttrain loss=0.000105\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=30\ttrain loss=0.000126\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=31\ttrain loss=0.000087\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=32\ttrain loss=0.000101\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=33\ttrain loss=0.000089\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=34\ttrain loss=0.000079\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=35\ttrain loss=0.000079\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=36\ttrain loss=0.000075\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=37\ttrain loss=0.000067\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=38\ttrain loss=0.000090\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=39\ttrain loss=0.000078\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=40\ttrain loss=0.000067\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=41\ttrain loss=0.000065\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=42\ttrain loss=0.000063\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=43\ttrain loss=0.000072\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=44\ttrain loss=0.000057\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=45\ttrain loss=0.000053\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=46\ttrain loss=0.000059\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=47\ttrain loss=0.000063\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=48\ttrain loss=0.000060\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=49\ttrain loss=0.000054\ttrain accuracy=1.000\ttest accuracy=0.938\n",
      "epoch=50\ttrain loss=0.000058\ttrain accuracy=1.000\ttest accuracy=0.938\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "for epoch in range(51):\n",
    "    running_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    for step, (batch_image, batch_label) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        # if torch.cuda.is_available():\n",
    "        #     batch_image, batch_label = batch_image.cuda(), batch_label.cuda()\n",
    "        batch_output = model(batch_image)\n",
    "        batch_loss = loss_func(batch_output, batch_label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += batch_loss.item()\n",
    "\n",
    "        # train accuracy\n",
    "        _, train_predicted = torch.max(batch_output.data, 1)\n",
    "        train_acc += (train_predicted == batch_label).sum().item()\n",
    "\n",
    "    train_acc /= train_size\n",
    "    running_loss /= (step+1)\n",
    "\n",
    "    # ----------test----------\n",
    "    model.eval()\n",
    "    test_acc = 0.0\n",
    "    for test_image, test_label in test_loader:\n",
    "        test_output = model(test_image)\n",
    "        _, predicted = torch.max(test_output.data, 1)\n",
    "        test_acc += (predicted == test_label).sum().item()\n",
    "    test_acc /= test_size\n",
    "\n",
    "    print('epoch={:d}\\ttrain loss={:.6f}\\ttrain accuracy={:.3f}\\ttest accuracy={:.3f}'.format(\n",
    "        epoch, running_loss, train_acc, test_acc))\n",
    "    if test_acc >= best_accuracy:\n",
    "        torch.save(model.state_dict(), 'FCNN_model.pkl')\n",
    "        best_accuracy = test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9bf5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
